{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSLkT0qL3jgl"
      },
      "source": [
        "# HW4P1: Language Modelling\n",
        "\n",
        "Welcome to the final part 1 hw of this course. This is the only part 1 in which you have PyTorch training (Yay). You will be working on training language models and evaluating them on the task of prediction and generation.<br>\n",
        "Note: A major change which we have made this semester is that we have made the model which you will be coding in this HW very similar to the Speller module from HW4P2. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "95e48c7693e34a389da49dcb6e448e0c",
        "deepnote_cell_type": "markdown",
        "id": "EB2bOV3bzYLR"
      },
      "source": [
        "# Get modules and datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4_-qG9rSULt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4529d814-9146-46c1-eade-38d400df2c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummaryX in /usr/local/lib/python3.9/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from torchsummaryX) (1.4.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from torchsummaryX) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchsummaryX) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->torchsummaryX) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->torchsummaryX) (2022.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->torchsummaryX) (3.10.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->torchsummaryX) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->torchsummaryX) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->torchsummaryX) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->torchsummaryX) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->torchsummaryX) (1.11.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->torchsummaryX) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->torchsummaryX) (16.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->torchsummaryX) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->torchsummaryX) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->torchsummaryX) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummaryX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnrUvEIC5i5j"
      },
      "outputs": [],
      "source": [
        "# TODO: Import drive if you are using Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "65e59cd2e6514d9594258167a6a0f6db",
        "deepnote_cell_type": "markdown",
        "id": "INh9p3v3zbF_"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "03bf3bd639a048f098d5febc42e2baff",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 4,
        "execution_start": 1679856365820,
        "id": "QZNwme4320LW",
        "source_hash": "b7876178"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "# sys.path.append(\"/content/drive/MyDrive/colab/11485/4\") # TODO: Add path to handout/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip handout.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEQcXjGiOh7_",
        "outputId": "0c99cbec-cb0f-4bf2-9f50-c760e4274896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  handout.zip\n",
            "replace __MACOSX/._handout? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "b48a9e95f26c4d2e89d95b1b311cedd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:09.992480Z",
          "iopub.status.busy": "2022-08-10T14:02:09.987693Z",
          "iopub.status.idle": "2022-08-10T14:02:12.872562Z",
          "shell.execute_reply": "2022-08-10T14:02:12.870819Z",
          "shell.execute_reply.started": "2022-08-10T14:02:09.991351Z"
        },
        "execution_millis": 2669,
        "execution_start": 1679856365830,
        "id": "oxiZ42B4SwQ-",
        "outputId": "1739b3e5-5169-4a66-f19d-88d800bad320",
        "source_hash": "ec149d26",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n",
            "/content/handout\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "\n",
        "import os\n",
        "\n",
        "import time \n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import torchsummaryX\n",
        "\n",
        "# Importing necessary modules from hw4\n",
        "from handout.hw4.tests_hw4 import test_prediction, test_generation\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device: \", DEVICE)\n",
        "\n",
        "%cd /content/handout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a4ff875589ee46da8f749a7e5088a3ef",
        "deepnote_cell_type": "markdown",
        "id": "u-R794-0zc9V"
      },
      "source": [
        "# Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU4e_6l0Whda",
        "outputId": "42ddee52-f402-43f9-b0ef-c50f4f6c09e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab length:  33280\n",
            "['!' '\"' '#' ... 'ï½ž' '<sos>' '<eos>']\n"
          ]
        }
      ],
      "source": [
        "# Loading the vocabulary. Try printing and see\n",
        "VOCAB       = np.load('dataset/vocab.npy') \n",
        "\n",
        "# We have also included <sos> and <eos> in the vocabulary for you\n",
        "# However in real life, you include it explicitly if not provided\n",
        "SOS_TOKEN   = np.where(VOCAB == '<sos>')[0][0]\n",
        "EOS_TOKEN   = np.where(VOCAB == '<eos>')[0][0]\n",
        "NUM_WORDS   = len(VOCAB) - 2 # Actual number of words in vocabulary\n",
        "\n",
        "print(\"Vocab length: \", len(VOCAB))\n",
        "print(VOCAB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA7SapmyXHr7"
      },
      "outputs": [],
      "source": [
        "# Loding the training dataset. Refer to write up section 2 to understand the structure\n",
        "dataset     = np.load('dataset/wiki.train.npy', allow_pickle=True)\n",
        "# The dataset does not have <sos> and <eos> because they are just regular articles. \n",
        "# TODO: Add <sos> and <eos> to every article in the dataset.\n",
        "# Before doing do, try printing the dataset to see if they are words or integers.\n",
        "for i in range(len(dataset)):\n",
        "  new_dataset_i = np.zeros(len(dataset[i])+2)\n",
        "  new_dataset_i[0] = SOS_TOKEN\n",
        "  new_dataset_i[-1] = EOS_TOKEN\n",
        "  new_dataset_i[1:-1] = dataset[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "09f3a2efaeef49ef9f4c0b2b9a614cca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:12.888156Z",
          "iopub.status.busy": "2022-08-10T14:02:12.884281Z",
          "iopub.status.idle": "2022-08-10T14:02:12.960590Z",
          "shell.execute_reply": "2022-08-10T14:02:12.958805Z",
          "shell.execute_reply.started": "2022-08-10T14:02:12.888058Z"
        },
        "execution_millis": 46,
        "execution_start": 1679856368507,
        "id": "x5znxQhLSwRC",
        "outputId": "5fba37f1-659d-4d93-f442-7b391daad03a",
        "source_hash": "42e4c03c",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation shapes    :  (128, 21) (128,)\n",
            "Test shapes          :  (128, 21)\n"
          ]
        }
      ],
      "source": [
        "# Loading the fixtures for validation and test - prediction\n",
        "fixtures_pred       = np.load('fixtures/prediction.npz')        # validation\n",
        "fixtures_pred_test  = np.load('fixtures/prediction_test.npz')   # test\n",
        "\n",
        "print(\"Validation shapes    : \", fixtures_pred['inp'].shape, fixtures_pred['out'].shape)\n",
        "print(\"Test shapes          : \", fixtures_pred_test['inp'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pes7mCr5WdAw",
        "outputId": "7a529095-3463-4628-f60b-5a457dcfa253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Gen Shapes    : (32, 21)\n",
            "Test Gen Shapes          : (128, 31)\n"
          ]
        }
      ],
      "source": [
        "# Loading the fixtures for validation and test - generation\n",
        "fixtures_gen        = np.load('fixtures/generation.npy')        # validation\n",
        "fixtures_gen_test   = np.load('fixtures/generation_test.npy')   # test\n",
        "\n",
        "print(\"Validation Gen Shapes    :\", fixtures_gen.shape)\n",
        "print(\"Test Gen Shapes          :\", fixtures_gen_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO_Qt7O6rL8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2d543c-5a4d-43bb-fb0a-50ea8a1ed1aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33278 26096 26972 25821 14658 29325 32935 21820 25639 16134 31353 29092\n",
            "    79  6916    76 21415 14658 24911  1424 29456 29325] 72\n"
          ]
        }
      ],
      "source": [
        "# Example Prediction Dev Input and Output\n",
        "# Optional TODO: You can try printing a few samples from the validation set which has both inputs and outputs\n",
        "print(fixtures_pred['inp'][0],fixtures_pred['out'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "aec0165a3f1245dfa52a0cb80dba2578",
        "deepnote_cell_type": "markdown",
        "id": "dHjYhXAOzkrP"
      },
      "source": [
        "# Custom DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "b2e63a7f6dec4a3f98588725a72a8ff2",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:13.079390Z",
          "iopub.status.busy": "2022-08-10T14:02:13.078847Z",
          "iopub.status.idle": "2022-08-10T14:02:13.196189Z",
          "shell.execute_reply": "2022-08-10T14:02:13.192167Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.079324Z"
        },
        "execution_millis": 48,
        "execution_start": 1679856368575,
        "id": "OZNrJ8XvSwRF",
        "source_hash": "a81eaa14",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class DataLoaderForLanguageModeling(torch.utils.data.DataLoader): # Inherit from torch.utils.data.DataLoader\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    # TODO: You can probably add more parameters as well. Eg. sequence length\n",
        "    def __init__(self, dataset, batch_size, seq_len, shuffle= True, drop_last= False): \n",
        "        \n",
        "        # If you remember, these are the standard things which you give while defining a dataloader.\n",
        "        # Now you are just customizing your dataloader\n",
        "        self.dataset    = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle    = shuffle\n",
        "        self.drop_last  = drop_last\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        # What output do you get when you print len(loader)? You get the number of batches\n",
        "        # Your dataset has (579, ) articles and each article has a specified amount of words.\n",
        "        # You concatenate the dataset and then batch parts of it according to the sequence length\n",
        "        # TODO: return the number of batches\n",
        "        # If you are using variable sequence_length, the length might not be fixed \n",
        "        num_words = 0\n",
        "        for i in range(len(self.dataset)):\n",
        "          num_words += len(self.dataset[i])\n",
        "        self.num_words = num_words\n",
        "        return int(np.ceil(self.num_words/(self.batch_size*self.seq_len)))\n",
        "\n",
        "    def __iter__(self):\n",
        "        # TODOs: \n",
        "        # 1. Shuffle data if shuffle is True\n",
        "        # 2. Concatenate articles and drop extra words\n",
        "        # 3. Divide the concetenated dataset into inputs and targets. How do they vary? \n",
        "        # 4. Reshape the inputs and targets into batches (think about the final shape)\n",
        "        # 5. Loop though the batches and yield the input and target according to the sequence length\n",
        "\n",
        "        if self.shuffle:\n",
        "          np.random.shuffle(self.dataset)\n",
        "\n",
        "        num_batches = self.__len__()\n",
        "        con_data = np.concatenate(self.dataset)\n",
        "\n",
        "        batch_idx = 0\n",
        "        if self.drop_last:\n",
        "            num_batches = num_batches if self.num_words % (self.batch_size*self.seq_len) == 0 else num_batches-1\n",
        "\n",
        "        while batch_idx < num_batches:\n",
        "            # print(len(con_data[batch_idx*self.batch_size*self.seq_len:(batch_idx+1)*self.batch_size*self.seq_len]))\n",
        "            input = torch.tensor(con_data[batch_idx*self.batch_size*self.seq_len:(batch_idx+1)*self.batch_size*self.seq_len].reshape(self.batch_size,self.seq_len))\n",
        "            target = torch.tensor(con_data[1+batch_idx*self.batch_size*self.seq_len:1+(batch_idx+1)*self.batch_size*self.seq_len].reshape(self.batch_size,self.seq_len))\n",
        "            batch_idx += 1\n",
        "            yield input, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "773573c8374048d4bcb5a67b905ee2e0",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 3,
        "execution_start": 1679856368714,
        "id": "fBZSzmy10M9M",
        "source_hash": "27952b8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "108ae1c8-a205-4aea-9469-ba9b7c81a1c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 10]) torch.Size([32, 10])\n",
            "x:  ['=', 'Djedkare', 'Isesi', '=', '<eol>', 'Djedkare', 'Isesi', '(', 'known', 'in']\n",
            "y:  ['Djedkare', 'Isesi', '=', '<eol>', 'Djedkare', 'Isesi', '(', 'known', 'in', 'Greek']\n",
            "x:  ['Greek', 'as', 'TancherÃªs', ')', 'was', 'an', 'Ancient', 'Egyptian', 'pharaoh', ',']\n",
            "y:  ['as', 'TancherÃªs', ')', 'was', 'an', 'Ancient', 'Egyptian', 'pharaoh', ',', 'the']\n",
            "x:  ['the', 'eighth', 'and', 'penultimate', 'ruler', 'of', 'the', 'Fifth', 'Dynasty', 'in']\n",
            "y:  ['eighth', 'and', 'penultimate', 'ruler', 'of', 'the', 'Fifth', 'Dynasty', 'in', 'the']\n"
          ]
        }
      ],
      "source": [
        "# Some sanity checks\n",
        "\n",
        "dl = DataLoaderForLanguageModeling(\n",
        "    dataset     = dataset, \n",
        "    batch_size  = 32, \n",
        "    shuffle     = True, \n",
        "    drop_last   = True,\n",
        "    seq_len = 10\n",
        ")\n",
        "\n",
        "inputs, targets = next(dl.__iter__())\n",
        "\n",
        "print(inputs.shape, targets.shape)\n",
        "\n",
        "for x, y in dl:\n",
        "    print(\"x: \", [VOCAB[i] for i in x[0, :]])\n",
        "    print(\"y: \", [VOCAB[i] for i in y[0, :]])\n",
        "    print(\"x: \", [VOCAB[i] for i in x[1, :]])\n",
        "    print(\"y: \", [VOCAB[i] for i in y[1, :]])\n",
        "    print(\"x: \", [VOCAB[i] for i in x[2, :]])\n",
        "    print(\"y: \", [VOCAB[i] for i in y[2, :]])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0e75c3c3318d481aa99230d81eb68c13",
        "deepnote_cell_type": "markdown",
        "id": "WcWU0YlnzmVM"
      },
      "source": [
        "# LanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TA Abuzar said it is ok to use this existing implementation of locked dropout\n",
        "# citation: https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/nn/lock_dropout.html\n",
        "import torch.nn as nn\n",
        "class LockedDropout(nn.Module):\n",
        "    \"\"\" LockedDropout applies the same dropout mask to every time step.\n",
        "\n",
        "    **Thank you** to Sales Force for their initial implementation of :class:`WeightDrop`. Here is\n",
        "    their `License\n",
        "    <https://github.com/salesforce/awd-lstm-lm/blob/master/LICENSE>`__.\n",
        "\n",
        "    Args:\n",
        "        p (float): Probability of an element in the dropout mask to be zeroed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (:class:`torch.FloatTensor` [sequence length, batch size, rnn hidden size]): Input to\n",
        "                apply dropout too.\n",
        "        \"\"\"\n",
        "        if not self.training or not self.p:\n",
        "            return x\n",
        "        x = x.clone()\n",
        "        mask = x.new_empty(1, x.size(1), x.size(2), requires_grad=False).bernoulli_(1 - self.p)\n",
        "        mask = mask.div_(1 - self.p)\n",
        "        mask = mask.expand_as(x)\n",
        "        return x * mask\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "            + 'p=' + str(self.p) + ')'"
      ],
      "metadata": {
        "id": "yd_BhnhbAAAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cebwoorWttWe"
      },
      "outputs": [],
      "source": [
        "# Here comes the main portion of this HW.\n",
        "# You can do this with a regular LSTM similar to HW3P2. \n",
        "# However, using LSTMCells will make this Language model very similar to the decoder in HW4P2 and we recommend you use that for writing resuable code.\n",
        "\n",
        "class LanguageModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size): # TODO: Add more parameters if you want\n",
        "        super().__init__()\n",
        "\n",
        "        # For all the layers which you will define, please read the documentation thoroughly before implementation\n",
        "\n",
        "        self.token_embedding    = torch.nn.Embedding(vocab_size,embedding_size) # TODO: Define a PyTorch embedding layer \n",
        "\n",
        "        self.lstm_cells         = torch.nn.Sequential(\n",
        "            torch.nn.LSTMCell(embedding_size,hidden_size), # TODO: Enter the parameters for the LSTMCells\n",
        "            # You can add multiple LSTMCells too if you want\n",
        "            torch.nn.LSTMCell(embedding_size,hidden_size),\n",
        "        )\n",
        "\n",
        "        self.token_probability  = torch.nn.Linear(hidden_size,vocab_size) # TODO: Define the parameters\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Optional TODO: Weight Tying. You just need to make the embedding layer weights equal to the Linear layer weight. \n",
        "        self.token_probability.weight = self.token_embedding.weight # weight tying\n",
        "        # So the basic pipline is:\n",
        "        # word -> embedding -> lstm -> projection (linear) to get probability distribution\n",
        "        # And this is happening across all time steps\n",
        "\n",
        "    def rnn_step(self, embedding, hidden_states_list):\n",
        "\n",
        "        for i in range(len(self.lstm_cells)):\n",
        "            # TODO: Forward pass through each LSTMCell\n",
        "            hidden_states_list[i] = self.lstm_cells[i](embedding, hidden_states_list[i])\n",
        "            embedding = hidden_states_list[i][0]\n",
        "            \n",
        "        return embedding, hidden_states_list\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Refer to Section 1.3.1 to understand this function\n",
        "        if not torch.is_tensor(x):\n",
        "            x = torch.tensor(x).long().to(DEVICE)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            # TODO: Pass the input sequence through the model \n",
        "            # and return the probability distribution of the last timestep\n",
        "            all_E = self.token_embedding(x) # E has shape (batch_size, seq_len, embedding_size)\n",
        "            hidden_states_list = [None]*len(self.lstm_cells)\n",
        "            for t in range(all_E.shape[1]):\n",
        "              E_t, hidden_states_list = self.rnn_step(all_E[:,t,:], hidden_states_list)\n",
        "            final_out = self.token_probability(E_t) # Get the probability distribution of the last timestep\n",
        "            return final_out\n",
        "\n",
        "    def generate(self, x, timesteps): \n",
        "        # Refer to section 1.3.2 to understand this function\n",
        "        # Important Note: We do not draw <eos> from the distribution unlike the writeup\n",
        "        if not torch.is_tensor(x):\n",
        "            x = torch.tensor(x).long().to(DEVICE)\n",
        "\n",
        "        # TODO: Pass the input sequence through the model \n",
        "        # Obtain the probability distribution and hidden_states_list of the last timestep\n",
        "        \n",
        "        token_prob_dist, hidden_states_list     = self.forward(x)\n",
        "        next_token                              = torch.argmax(token_prob_dist,dim=2)[:,-1] # TODO: Draw the next predicted token from the probability distribution\n",
        "\n",
        "        generated_sequence  = [] \n",
        "\n",
        "        with torch.inference_mode():\n",
        "            for t in range(timesteps): # Loop through the timesteps\n",
        "                #   TODO: Pass the next_token and hidden_states_list through the model\n",
        "                #   TODO: You will get 2 outputs. What is the shape of the probability distribution?\n",
        "                #   TODO: Get the most probable token for the next timestep\n",
        "                E_t = self.token_embedding(next_token)\n",
        "                E_t, hidden_states_list = self.rnn_step(E_t, hidden_states_list)\n",
        "                token_prob_dist = self.token_probability(E_t)\n",
        "                next_token = torch.argmax(token_prob_dist,dim=1)\n",
        "                generated_sequence.append(next_token)\n",
        "            \n",
        "            generated_sequence = torch.stack(generated_sequence, dim=1) # keep last timesteps generated words\n",
        "\n",
        "        return generated_sequence\n",
        "\n",
        "    # We are also having a hidden_states_list parameter because you need that in generation\n",
        "    def forward(self, x, hidden_states_list= None): # train model\n",
        "        # x (Batch, Seq_len)\n",
        "        # Note: you dont have to return the sum of log probabilities according to Pseudocode 1 in the writeup\n",
        "        # However, feel free to calculate and print it if you are curious\n",
        "\n",
        "        batch_size, timesteps   = x.shape \n",
        "\n",
        "        token_prob_distribution = [] # list which will contain probability distributions for all timesteps\n",
        "        # Initializing the hidden states\n",
        "        hidden_states_list      = [None]*len(self.lstm_cells) if hidden_states_list == None else hidden_states_list       \n",
        "\n",
        "        token_embeddings        = self.token_embedding(x)\n",
        "        ldp = LockedDropout(p=0.3)\n",
        "        token_embeddings = ldp(token_embeddings)\n",
        "        # When you get the embeddings of the input x, remember that you get it for all time steps.\n",
        "        # Embedding is just a linear transformation so you can precompute it for all time steps.\n",
        "\n",
        "        for t in range(timesteps): # LSTMCell is for just 1 timestep. Hence you need to loop through the total timesteps\n",
        "\n",
        "            token_embedding_t           = token_embeddings[:,t,:]\n",
        "\n",
        "            rnn_out, hidden_states_list = self.rnn_step(token_embedding_t, hidden_states_list)\n",
        "            \n",
        "            token_prob_dist_t           = self.token_probability(rnn_out)\n",
        "\n",
        "            token_prob_distribution.append(token_prob_dist_t) \n",
        "\n",
        "        token_prob_distribution = torch.stack(token_prob_distribution, dim=1) # TODO: Stack along the timesteps dimension\n",
        "\n",
        "        return token_prob_distribution, hidden_states_list # prob should be of shape (B, T, Vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8ed5a6ef54f9446fab752b79c70a0216",
        "deepnote_cell_type": "markdown",
        "id": "TlWF_bpLznup"
      },
      "source": [
        "# Trainer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "8ea986fc372643389d1ab4c445659e9d",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:13.440820Z",
          "iopub.status.busy": "2022-08-10T14:02:13.440281Z",
          "iopub.status.idle": "2022-08-10T14:02:13.644455Z",
          "shell.execute_reply": "2022-08-10T14:02:13.642614Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.440752Z"
        },
        "id": "kIvZOIfjSwRK",
        "source_hash": "451a140f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Unlike all the P2s, we are using a Trainer class for this HW.\n",
        "# Many researchers also use classes like this for training. You may have encountered them in your project as well.\n",
        "# You dont have to complete everything in this class, you only need to complete the train function.\n",
        "# However, its good to go through the code and see what it does. \n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "class Trainer:\n",
        "    def __init__(self, model, loader, optimizer, criterion, scheduler, max_epochs= 1, run_id= 'exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model      = model\n",
        "        self.loader     = loader\n",
        "        self.optimizer  = optimizer\n",
        "        self.criterion  = criterion\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "        self.train_losses           = []\n",
        "        self.val_losses             = []\n",
        "        self.predictions            = []\n",
        "        self.predictions_test       = []\n",
        "        self.generated_logits       = []\n",
        "        self.generated              = []\n",
        "        self.generated_logits_test  = []\n",
        "        self.generated_test         = []\n",
        "        self.epochs                 = 0\n",
        "        self.max_epochs             = max_epochs\n",
        "        self.run_id                 = run_id\n",
        "\n",
        "\n",
        "    def calculate_loss(self, out, target):\n",
        "        # output: (B, T, Vocab_size) - probability distributions\n",
        "        # target: (B, T)\n",
        "        # Read the documentation of CrossEntropyLoss and try to understand how it takes inputs\n",
        "\n",
        "        # Tip: If your target is of shape (B, T) it means that you have B batches with T words. \n",
        "        # Tip: What is the total number of words in this batch? \n",
        "        # Tip: Crossentropy calculates the loss between a label and its probability distribution.\n",
        "        out     = torch.reshape(out,(out.shape[0]*out.shape[1],out.shape[2])).to(torch.float32)\n",
        "        target = torch.reshape(target,(target.shape[0]*target.shape[1],)).to(torch.int64)\n",
        "        loss    = self.criterion(out, target)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        self.model.train() # set to training mode\n",
        "        self.model.to(DEVICE)\n",
        "        epoch_loss  = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        for batch_num, (inputs, targets) in enumerate(tqdm(self.loader)):\n",
        "\n",
        "            # TODO: Complete the loop. You should be able to complete this without any helper comments after 3 HWs\n",
        "            # Tip: Mixed precision training\n",
        "            # For loss calculation, use the calculate_loss function. You need to complete it before using.\n",
        "            self.optimizer.zero_grad()\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "            # mixed precision forward prop\n",
        "            with torch.cuda.amp.autocast():\n",
        "              outputs_prob, hidden_states_list = self.model(inputs)\n",
        "              loss    = self.calculate_loss(outputs_prob, targets)\n",
        "            # back prop\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # gradient descent\n",
        "            scaler.step(optimizer)\n",
        "\n",
        "            scaler.update()\n",
        "            loss = loss.item()\n",
        "            epoch_loss += loss\n",
        "        \n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        self.epochs += 1\n",
        "        print('[TRAIN] \\tEpoch [%d/%d] \\tLoss: %.4f \\tLr: %.6f'\n",
        "                      % (self.epochs, self.max_epochs, epoch_loss, self.optimizer.param_groups[0]['lr']))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "        self.scheduler.step()\n",
        "\n",
        "\n",
        "    \n",
        "    def test(self): # Don't change this function\n",
        "        \n",
        "        self.model.eval() # set to eval mode\n",
        "        predictions     = model.predict(fixtures_pred['inp']).detach().cpu().numpy() # get predictions\n",
        "        self.predictions.append(predictions)\n",
        "\n",
        "        generated_logits        = model.generate(fixtures_gen, 10).detach().cpu().numpy() # generated predictions for 10 words\n",
        "        generated_logits_test   = model.generate(fixtures_gen_test, 10).detach().cpu().numpy()\n",
        "\n",
        "        nll             = test_prediction(predictions, fixtures_pred['out'])\n",
        "        generated       = test_generation(fixtures_gen, generated_logits, VOCAB)\n",
        "        generated_test  = test_generation(fixtures_gen_test, generated_logits_test, VOCAB)\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        self.generated.append(generated)\n",
        "        self.generated_test.append(generated_test)\n",
        "        self.generated_logits.append(generated_logits)\n",
        "        self.generated_logits_test.append(generated_logits_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        predictions_test = model.predict(fixtures_pred_test['inp']).detach().cpu().numpy() # get predictions\n",
        "        self.predictions_test.append(predictions_test)\n",
        "            \n",
        "        print('[VAL] \\tEpoch [%d/%d] \\tLoss: %.4f'\n",
        "                      % (self.epochs, self.max_epochs, nll))\n",
        "        return nll\n",
        "\n",
        "    \n",
        "    def save(self): # Don't change this function\n",
        "\n",
        "        model_path = os.path.join('hw4/experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()}, model_path)\n",
        "        np.save(os.path.join('hw4/experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "        np.save(os.path.join('hw4/experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "        np.save(os.path.join('hw4/experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "        np.save(os.path.join('hw4/experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "        \n",
        "        with open(os.path.join('hw4/experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated[-1])\n",
        "\n",
        "        with open(os.path.join('hw4/experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_test[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "db5de3ac0c6e48ca9cff0cd79bef7ae8",
        "deepnote_cell_type": "markdown",
        "id": "E6NKG0j8zsv-"
      },
      "source": [
        "# Experiment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "7fc44ee4771a42f996d0a00d35529fb6",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:13.852171Z",
          "iopub.status.busy": "2022-08-10T14:02:13.850633Z",
          "iopub.status.idle": "2022-08-10T14:02:13.927227Z",
          "shell.execute_reply": "2022-08-10T14:02:13.924500Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.852093Z"
        },
        "id": "TiUrjbEjSwRQ",
        "source_hash": "f7524436",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TODO: define other hyperparameters here\n",
        "\n",
        "configs = dict(\n",
        "    batch_size  = 64,\n",
        "    num_epochs  = 20, # 10 or 20 epochs should be enough given the model is good\n",
        "    init_lr     = 1e-3,\n",
        "    hidden_size = 400,\n",
        "    embedding_size = 400,\n",
        "    vocab_size = len(VOCAB),\n",
        "    seq_len = 10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "4aaccf1c32fa480a9a15e8bb8bc4d9e4",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:14.110787Z",
          "iopub.status.busy": "2022-08-10T14:02:14.109778Z",
          "iopub.status.idle": "2022-08-10T14:02:14.929087Z",
          "shell.execute_reply": "2022-08-10T14:02:14.925078Z",
          "shell.execute_reply.started": "2022-08-10T14:02:14.110707Z"
        },
        "id": "DbHH6zXTSwRa",
        "source_hash": "2acff566",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model       = LanguageModel(vocab_size=configs['vocab_size'], hidden_size=configs['hidden_size'], embedding_size=configs['embedding_size']).to(DEVICE)\n",
        "\n",
        "loader      = DataLoaderForLanguageModeling(dataset=dataset,batch_size=configs['batch_size'],shuffle=True,drop_last=True,seq_len=configs['seq_len']) # TODO: Define the dataloader\n",
        "\n",
        "criterion   = torch.nn.CrossEntropyLoss(reduction='mean') \n",
        "optimizer   = torch.optim.AdamW(model.parameters(), lr=configs['init_lr']) # TODO: Define the optimizer. Adam/AdamW usually works good for this HW\n",
        "\n",
        "# Optional TODO: Use a scheduler if you want\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,1,0.9)\n",
        "\n",
        "# print(model)\n",
        "# torchsummaryX.summary(model, x = inputs.to(DEVICE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "aaff53cf948e44b7b9bd49cbcad0ac58",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:13.931258Z",
          "iopub.status.busy": "2022-08-10T14:02:13.930204Z",
          "iopub.status.idle": "2022-08-10T14:02:14.107883Z",
          "shell.execute_reply": "2022-08-10T14:02:14.105987Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.931185Z"
        },
        "id": "2HCVG5YISwRW",
        "source_hash": "c9f4594a",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa55f44-63c3-49e5-b6db-e95d6ab7db3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving models, predictions, and generated words to ./hw4/experiments/1682009836\n"
          ]
        }
      ],
      "source": [
        "# Dont change this cell\n",
        "\n",
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./hw4/experiments'):\n",
        "    os.mkdir('./hw4/experiments')\n",
        "os.mkdir('./hw4/experiments/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to ./hw4/experiments/%s\" % run_id)\n",
        "\n",
        "# The object of the Trainer class takes in everything\n",
        "trainer = Trainer(\n",
        "    model       = model, \n",
        "    loader      = loader, \n",
        "\n",
        "    optimizer   = optimizer,\n",
        "    criterion   = criterion, \n",
        "    \n",
        "    max_epochs  = configs['num_epochs'], \n",
        "    run_id      = run_id,\n",
        "    scheduler = scheduler\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0dy4CpXJgN2"
      },
      "outputs": [],
      "source": [
        "# Run the experiments loop. \n",
        "# Each epoch wont take more than 2-3min. If its taking more time, it might be due to (but not limited to) the following:\n",
        "#   * You might be overlapping batches \n",
        "#       Eg. Input: \"I had biryani for lunch today\" and sequence length = 3,\n",
        "#           --> \"I had biryani\", \"for lunch today\" are ideal examples for inputs\n",
        "#           --> \"I had biryani\", \"had biryani for\", \"biryani for lunch\", ... is just redundant info :')\n",
        "#   * Your length calculation in the dataloader might be wrong\n",
        "# If you haven't had biryani, try it :D \n",
        "\n",
        "%%time\n",
        "best_nll = 1e30 \n",
        "for epoch in range(configs['num_epochs']):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch+1)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_gYqXq9Jgo1"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses[0:10], label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBqjqy-EyU27"
      },
      "source": [
        "# Create handin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MASzI7jpySYF",
        "outputId": "66988424-33fa-40e2-9cbd-5497b3689c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "11-785-s23-hw3p2.zip  \u001b[0m\u001b[01;34mdata\u001b[0m/     handout.zip  \u001b[01;34msample_data\u001b[0m/\n",
            "\u001b[01;34mctcdecode\u001b[0m/            \u001b[01;34mhandout\u001b[0m/  \u001b[01;34m__MACOSX\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(configs['num_epochs'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hxKdd8IzVzr",
        "outputId": "7a490890-2264-4e55-c87e-be5e00a2450f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(run_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4mZxHogz94q",
        "outputId": "55fe1bf2-25aa-4a93-ddfd-3ab8d9926057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1682009836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1qCDVocHnDF",
        "outputId": "e80ccec4-ea33-4cef-b724-8f0c42205af1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/handout\n",
            "\u001b[0m\u001b[01;34mdataset\u001b[0m/  \u001b[01;34mfixtures\u001b[0m/  handin.tar  \u001b[01;34mhw4\u001b[0m/  Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWRyPvWmgLQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e404b85d-e45b-4c76-c2da-ddd62a50f534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp hw4/experiments/1682009836/predictions-test-10.npy predictions.npy\n",
            "cp hw4/experiments/1682009836/generated-10.txt generated.txt\n",
            "cp hw4/experiments/1682009836/generated_logits-test-10.npy generated_logits.npy\n",
            "cp hw4/hw4p1.ipynb training.ipynb\n",
            "tar -cvf handin.tar training.ipynb predictions.npy generated.txt generated_logits.npy\n",
            "training.ipynb\n",
            "predictions.npy\n",
            "generated.txt\n",
            "generated_logits.npy\n",
            "rm -f generated.txt predictions.npy training.ipynb generated_logits.npy\n"
          ]
        }
      ],
      "source": [
        "# TODO: Generate the handin to submit to autolab\n",
        "!make runid=1682009836 epoch=10"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9HMSxtJTvK9Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "989a3c3836794109ac641230122845a3",
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}